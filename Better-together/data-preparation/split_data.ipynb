{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to perform a sanity check and split the dataset into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from pykeen import datasets\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import argparse, sys\n",
    "from loguru import logger\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 168M/168M [03:48<00:00, 736kB/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to /home/ahmadi/sadaf/GraphNeighborLM/Better-together/data-preparation/wikidata5m_transductive.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define your specific path\n",
    "save_path = \"/home/ahmadi/sadaf/GraphNeighborLM/Better-together/data-preparation\"\n",
    "os.makedirs(save_path, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# File URL and destination\n",
    "url = \"https://www.dropbox.com/s/6sbhm0rwo4l73jq/wikidata5m_transductive.tar.gz?dl=1\"\n",
    "file_path = os.path.join(save_path, \"wikidata5m_transductive.tar.gz\")\n",
    "\n",
    "# Download with progress\n",
    "def download_with_progress(url, file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        total_size = int(response.info().get('Content-Length', 0))  # Total size in bytes\n",
    "        block_size = 1024  # Block size in bytes\n",
    "        t = tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            while True:\n",
    "                buffer = response.read(block_size)\n",
    "                if not buffer:\n",
    "                    break\n",
    "                f.write(buffer)\n",
    "                t.update(len(buffer))\n",
    "        t.close()\n",
    "\n",
    "print(\"Downloading file...\")\n",
    "download_with_progress(url, file_path)\n",
    "print(f\"File saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the tar.gz file...\n",
      "Files extracted to /home/ahmadi/sadaf/GraphNeighborLM/Better-together/data-preparation/datasets/wikidata5m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the dataset directory\n",
    "dataset_path = os.path.join(save_path, \"datasets/wikidata5m\")\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "# File paths\n",
    "tar_file_path = os.path.join(save_path, \"wikidata5m_transductive.tar.gz\")\n",
    "\n",
    "# Extract the tar.gz file\n",
    "print(\"Extracting the tar.gz file...\")\n",
    "with tarfile.open(tar_file_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=dataset_path)\n",
    "print(f\"Files extracted to {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/ahmadi/sadaf/GraphNeighborLM/Better-together/data-preparation/datasets/wikidata5m/wikidata5m_transductive_train.txt\"\n",
    "test_path = \"/home/ahmadi/sadaf/GraphNeighborLM/Better-together/data-preparation/datasets/wikidata5m/wikidata5m_transductive_test.txt\"\n",
    "valid_path = \"/home/ahmadi/sadaf/GraphNeighborLM/Better-together/data-preparation/datasets/wikidata5m/wikidata5m_transductive_valid.txt\"\n",
    "\n",
    "# Load the datasets\n",
    "columns = [\"head\", \"relation\", \"tail\"]  \n",
    "train_df = pd.read_csv(train_path, sep='\\t', header=None, names=columns)\n",
    "test_df = pd.read_csv(test_path, sep='\\t', header=None, names=columns)\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', header=None, names=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>relation</th>\n",
       "      <th>tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q29387131</td>\n",
       "      <td>P31</td>\n",
       "      <td>Q5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q326660</td>\n",
       "      <td>P1412</td>\n",
       "      <td>Q652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q7339549</td>\n",
       "      <td>P57</td>\n",
       "      <td>Q1365729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q554335</td>\n",
       "      <td>P27</td>\n",
       "      <td>Q29999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q20641639</td>\n",
       "      <td>P54</td>\n",
       "      <td>Q80955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        head relation      tail\n",
       "0  Q29387131      P31        Q5\n",
       "1    Q326660    P1412      Q652\n",
       "2   Q7339549      P57  Q1365729\n",
       "3    Q554335      P27    Q29999\n",
       "4  Q20641639      P54    Q80955"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train dataset:  (20614279, 3)\n",
      "shape of valid dataset:  (5163, 3)\n",
      "shape of test dataset:  (5133, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of train dataset: \", train_df.shape)\n",
    "print(\"shape of valid dataset: \", valid_df.shape)\n",
    "print(\"shape of test dataset: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20624575, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate all datasets\n",
    "all_data = pd.concat([train_df, test_df, valid_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the data\n",
    "all_data = all_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Data:\n",
      "        head relation      tail\n",
      "0   Q6442440      P31    Q23397\n",
      "1   Q7148490      P17       Q30\n",
      "2  Q16145304      P19  Q3752988\n",
      "3   Q5201498     P421     Q6723\n",
      "4    Q937515    P1344     Q9674\n"
     ]
    }
   ],
   "source": [
    "# Select 200 samples\n",
    "subset_data = all_data.iloc[:700]\n",
    "\n",
    "# Display the subset\n",
    "print(\"Subset Data:\")\n",
    "print(subset_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 500\n",
      "Validation Size: 100\n",
      "Test Size: 100\n"
     ]
    }
   ],
   "source": [
    "# Split into train, validation, and test sets\n",
    "train_data = subset_data.iloc[:500]\n",
    "val_data = subset_data.iloc[500:600]\n",
    "test_data = subset_data.iloc[600:700]\n",
    "\n",
    "# Display sizes of splits\n",
    "print(f\"Train Size: {len(train_data)}\")\n",
    "print(f\"Validation Size: {len(val_data)}\")\n",
    "print(f\"Test Size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsets saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the splits to files\n",
    "train_data.to_csv(f\"{dataset_path}/subset_train.txt\", sep='\\t', index=False, header=False)\n",
    "val_data.to_csv(f\"{dataset_path}/subset_val.txt\", sep='\\t', index=False, header=False)\n",
    "test_data.to_csv(f\"{dataset_path}/subset_test.txt\", sep='\\t', index=False, header=False)\n",
    "print(\"Subsets saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths\n",
    "train_path = \"/home/ahmadi/sadaf/GraphNeighborLM/Better-together/data-preparation/datasets/wikidata5m/subset_train.txt\"\n",
    "test_path = \"/home/ahmadi/sadaf/GraphNeighborLM/Better-together/data-preparation/datasets/wikidata5m/subset_test.txt\"\n",
    "valid_path = \"/home/ahmadi/sadaf/GraphNeighborLM/Better-together/data-preparation/datasets/wikidata5m/subset_val.txt\"\n",
    "\n",
    "# Load the datasets\n",
    "columns = [\"head\", \"relation\", \"tail\"]  # Adjust column names if necessary\n",
    "train_df = pd.read_csv(train_path, sep='\\t', header=None, names=columns)\n",
    "test_df = pd.read_csv(test_path, sep='\\t', header=None, names=columns)\n",
    "valid_df = pd.read_csv(valid_path, sep='\\t', header=None, names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>relation</th>\n",
       "      <th>tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q6442440</td>\n",
       "      <td>P31</td>\n",
       "      <td>Q23397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q7148490</td>\n",
       "      <td>P17</td>\n",
       "      <td>Q30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q16145304</td>\n",
       "      <td>P19</td>\n",
       "      <td>Q3752988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q5201498</td>\n",
       "      <td>P421</td>\n",
       "      <td>Q6723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q937515</td>\n",
       "      <td>P1344</td>\n",
       "      <td>Q9674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Q8270170</td>\n",
       "      <td>P31</td>\n",
       "      <td>Q5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Q16977847</td>\n",
       "      <td>P407</td>\n",
       "      <td>Q1860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Q113008</td>\n",
       "      <td>P1412</td>\n",
       "      <td>Q188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Q24928114</td>\n",
       "      <td>P17</td>\n",
       "      <td>Q668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Q6696610</td>\n",
       "      <td>P31</td>\n",
       "      <td>Q486972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          head relation      tail\n",
       "0     Q6442440      P31    Q23397\n",
       "1     Q7148490      P17       Q30\n",
       "2    Q16145304      P19  Q3752988\n",
       "3     Q5201498     P421     Q6723\n",
       "4      Q937515    P1344     Q9674\n",
       "..         ...      ...       ...\n",
       "495   Q8270170      P31        Q5\n",
       "496  Q16977847     P407     Q1860\n",
       "497    Q113008    P1412      Q188\n",
       "498  Q24928114      P17      Q668\n",
       "499   Q6696610      P31   Q486972\n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Verbalizer:\n",
    "    def __init__(self, df, similarity_matrix=None, relation2index=None, entity2text=None, relation2text=None):\n",
    "        self.df = df\n",
    "        self.similarity_matrix = similarity_matrix\n",
    "        self.relation2index = relation2index\n",
    "        self.entity2text = entity2text\n",
    "        self.relation2text = relation2text\n",
    "        self.sep = '[SEP]'\n",
    "\n",
    "    def get_neighbourhood(self, node_id, relation_id=None, tail_id=None, limit=None):\n",
    "        neighs = []\n",
    "\n",
    "        if tail_id is None:\n",
    "            # Neighbors for test dataset\n",
    "            head_matches = self.df[self.df['head'] == node_id].copy()\n",
    "            if limit:\n",
    "                head_matches = head_matches.head(limit)\n",
    "            neighs.extend(head_matches.to_dict('records'))\n",
    "\n",
    "            tail_matches = self.df[self.df['tail'] == node_id].copy()\n",
    "            if limit:\n",
    "                tail_matches = tail_matches.head(limit)\n",
    "            for _, row in tail_matches.iterrows():\n",
    "                row = row.to_dict()\n",
    "                row['relation'] = 'inverse of ' + row['relation']\n",
    "                neighs.append(row)\n",
    "        else:\n",
    "            # Neighbors for train dataset\n",
    "            head_matches = self.df[(self.df['head'] == node_id) &\n",
    "                                   ((self.df['tail'] != tail_id) |\n",
    "                                    (self.df['relation'] != relation_id))].copy()\n",
    "            if limit:\n",
    "                head_matches = head_matches.head(limit)\n",
    "            neighs.extend(head_matches.to_dict('records'))\n",
    "\n",
    "            tail_matches = self.df[(self.df['tail'] == node_id) &\n",
    "                                   ((self.df['head'] != tail_id) |\n",
    "                                    (self.df['relation'] != relation_id))].copy()\n",
    "            if limit:\n",
    "                tail_matches = tail_matches.head(limit)\n",
    "            for _, row in tail_matches.iterrows():\n",
    "                row = row.to_dict()\n",
    "                row['relation'] = 'inverse of ' + row['relation']\n",
    "                neighs.append(row)\n",
    "\n",
    "        return neighs\n",
    "\n",
    "    def verbalize(self, head, relation, tail=None, inverse=False):\n",
    "        relation_prefix = 'inverse of ' if inverse else ''\n",
    "        limit = 200 if inverse else None\n",
    "\n",
    "        neighbourhood = self.get_neighbourhood(head, relation, tail, limit)\n",
    "        relation_text = relation_prefix + self.relation2text[relation]\n",
    "\n",
    "        # Sort based on similarity\n",
    "        neighbourhood.sort(\n",
    "            key=lambda x: self.similarity_matrix[self.relation2index[self.relation2text[x['relation']]]]\n",
    "                         [self.relation2index[relation_text]],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        neighbourhood = neighbourhood[:512]\n",
    "        verbalization = f\"predict {self.sep} {self.entity2text[head]} {relation_text} {self.sep} \"\n",
    "\n",
    "        verbalization += \" \".join(\n",
    "            list(\n",
    "                map(\n",
    "                    lambda x: f\"{self.relation2text[x['relation']]} \" +\n",
    "                              f\"{self.entity2text[x['tail']] if x['head'] == head else self.entity2text[x['head']]} {self.sep}\",\n",
    "                    neighbourhood\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return \" \".join(verbalization.split()).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbalize_dataset(input_df, verbalizer):\n",
    "    docs = []\n",
    "\n",
    "    for i, doc in tqdm(input_df.iterrows(), total=len(input_df)):\n",
    "        try:\n",
    "            direct_verbalization = verbalizer.verbalize(doc['head'], doc['relation'], doc['tail'])\n",
    "            docs.append({\n",
    "                'id': i * 2,\n",
    "                'verbalization': direct_verbalization,\n",
    "                'head': doc['head'],\n",
    "                'tail': doc['tail'],\n",
    "                'relation': doc['relation'],\n",
    "                'verbalized_tail': verbalizer.entity2text[doc['tail']]\n",
    "            })\n",
    "\n",
    "            inverse_verbalization = verbalizer.verbalize(doc['tail'], doc['relation'], doc['head'], inverse=True)\n",
    "            docs.append({\n",
    "                'id': i * 2 + 1,\n",
    "                'verbalization': inverse_verbalization,\n",
    "                'head': doc['tail'],\n",
    "                'tail': doc['head'],\n",
    "                'relation': \"inverse of \" + doc['relation'],\n",
    "                'verbalized_tail': verbalizer.entity2text[doc['head']]\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Exception {e} on {i}th triplet\")\n",
    "\n",
    "    return pd.DataFrame(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             [--relation_vectors_path RELATION_VECTORS_PATH]\n",
      "                             [--rel2ind_path REL2IND_PATH]\n",
      "                             [--entity_mapping_path ENTITY_MAPPING_PATH]\n",
      "                             [--relation_mapping_path RELATION_MAPPING_PATH]\n",
      "                             [--train_path TRAIN_PATH]\n",
      "                             [--valid_path VALID_PATH] [--test_path TEST_PATH]\n",
      "                             [--train_verbalized_output TRAIN_VERBALIZED_OUTPUT]\n",
      "                             [--valid_verbalized_output VALID_VERBALIZED_OUTPUT]\n",
      "                             [--test_verbalized_output TEST_VERBALIZED_OUTPUT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/ahmadi/.local/share/jupyter/runtime/kernel-v3d7530335ff9860d7196d5148d23a01e479dfe3c3.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/hd1/users/ahmadi/sadaf/GraphNeighborLM/sadafenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--relation_vectors_path\", help=\"path to the embeddings of verbalized relations\", default=\"/home/ahmadi/sadaf/GraphNeighborLM/Better-together/data-preparation/data/embeddings/fasttext_vecs-wikidata5m.npy\")\n",
    "parser.add_argument(\"--rel2ind_path\", help=\"path to the mapping of textual relations to the index of corresponding vectors\", default=\"/home/ahmadi/sadaf/GraphNeighborLM/Better-together/data-preparation/data/relation2ind-wikidata5m.json\")\n",
    "parser.add_argument(\"--entity_mapping_path\", help=\"path to the entity2text mapping\", default=\"/home/ahmadi/sadaf/GraphNeighborLM/Better-together/data-preparation/data/mappings/wd5m_aliases_entities_v3.txt\")\n",
    "parser.add_argument(\"--relation_mapping_path\", help=\"path to the relation2text mapping\", default=\"/home/ahmadi/sadaf/GraphNeighborLM/Better-together/data-preparation/data/relation2text-wikidata5m.json\")\n",
    "parser.add_argument(\"--train_path\", help=\"train KG path\", default='/home/ahmadi/sadaf/GraphNeighborLM/Better-together/data-preparation/datasets/wikidata5m/subset_train.txt')\n",
    "parser.add_argument(\"--valid_path\", help=\"valid KG path\", default='/home/ahmadi/sadaf/GraphNeighborLM/Better-together/data-preparation/datasets/wikidata5m/subset_val.txt')\n",
    "parser.add_argument(\"--test_path\", help=\"test KG path\", default='/home/ahmadi/sadaf/GraphNeighborLM/Better-together/data-preparation/datasets/wikidata5m/subset_test.txt')\n",
    "parser.add_argument(\"--train_verbalized_output\", help=\"verbalized train KG path\", default='verbalized_train')\n",
    "parser.add_argument(\"--valid_verbalized_output\", help=\"verbalized valid KG path\", default='verbalized_valid')\n",
    "parser.add_argument(\"--test_verbalized_output\", help=\" verbalized test KG path\", default='verbalized_test')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = np.load(args.relation_vectors_path)\n",
    "similarity_matrix = cosine_similarity(vecs)\n",
    "\n",
    "with open(args.rel2ind_path, 'r') as f:\n",
    "    rel2ind = json.load(f)\n",
    "\n",
    "entity_mapping = {}\n",
    "with open(args.entity_mapping_path, 'r') as f:\n",
    "    for line in f:\n",
    "        _id, name = line.strip().split('\\t')\n",
    "        entity_mapping[_id] = name\n",
    "\n",
    "with open(args.relation_mapping_path, 'r') as f:\n",
    "    relation_mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(args.train_path, sep='\\t', header=None, names=columns)\n",
    "test_df = pd.read_csv(args.test_path, sep='\\t', header=None, names=columns)\n",
    "valid_df = pd.read_csv(args.valid_path, sep='\\t', header=None, names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbalize datasets\n",
    "verbalizer = Verbalizer(train_df, similarity_matrix=similarity_matrix,\n",
    "                        relation2index=rel2ind, entity2text=entity_mapping, relation2text=relation_mapping)\n",
    "\n",
    "print('Verbalizing train KG...')\n",
    "train_verbalized = verbalize_dataset(train_df, verbalizer)\n",
    "\n",
    "print('Verbalizing valid KG...')\n",
    "valid_verbalized = verbalize_dataset(valid_df, verbalizer)\n",
    "\n",
    "print('Verbalizing test KG...')\n",
    "test_verbalized = verbalize_dataset(test_df, verbalizer)\n",
    "\n",
    "# Save results\n",
    "train_verbalized.to_csv(args.train_verbalized_output, index=False)\n",
    "valid_verbalized.to_csv(args.valid_verbalized_output, index=False)\n",
    "test_verbalized.to_csv(args.test_verbalized_output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sadafenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
