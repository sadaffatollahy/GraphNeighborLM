{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v-eOOeySlUy"
      },
      "outputs": [],
      "source": [
        "import contextlib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda.amp import autocast as autocast\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch_scatter import scatter\n",
        "#from src.model.gnn import load_gnn_model\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    # prepare_model_for_int8_training,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "\n",
        "BOS = '<s>[INST]'  #marks the start of an instruction-based input for the language model.\n",
        "EOS_USER = '[/INST]' #marks the end of the user input or instruction in an instruction-based input format.\n",
        "EOS = '</s>' #mark the completion of text generation by the model.\n",
        "\n",
        "IGNORE_INDEX = -100 #mask non-target tokens during loss computation.loss functions (e.g., CrossEntropyLoss) ignore positions marked with IGNORE_INDEX\n",
        "                    # used to mask padding positions during loss computation.\n",
        "\n",
        "class GraphLLM(torch.nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.max_txt_len = args.max_txt_len #maximum length of input text sequences\n",
        "        self.max_new_tokens = args.max_new_tokens #maximum number of tokens the model is allowed to generate during inference.\n",
        "\n",
        "        print('Loading LLAMA')\n",
        "        kwargs = {\n",
        "            #\"max_memory\": {0: '80GiB', 1: '80GiB'},\n",
        "            \"device_map\": \"auto\", #Automatically maps the modelâ€™s layers across available devices for efficient training/inference.\n",
        "            \"revision\": \"main\", #Specifies the model version or branch to use\n",
        "        }\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(args.llm_model_path,  revision=kwargs[\"revision\"])\n",
        "        self.tokenizer.pad_token_id = 0\n",
        "        self.tokenizer.padding_side = 'left'\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            args.llm_model_path,\n",
        "            torch_dtype=torch.float16, #used for reduce memory\n",
        "            low_cpu_mem_usage=True, #used for reduce memory\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        ###LORA configuration\n",
        "\n",
        "        if args.llm_frozen == 'True':\n",
        "            print(\"Freezing LLAMA!\")\n",
        "            for name, param in model.named_parameters():\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            print(\"Training LLAMA with LORA!\")\n",
        "            # model = prepare_model_for_int8_training(model)\n",
        "            model = prepare_model_for_kbit_training(model)#Reduces the memory footprint while retaining sufficient precision for effective fine-tuning.\n",
        "            lora_r: int = 8\n",
        "            lora_alpha: int = 16\n",
        "            lora_dropout: float = 0.05\n",
        "            lora_target_modules = [ #Specifies which modules to adapt\n",
        "                \"q_proj\",\n",
        "                \"v_proj\",\n",
        "            ]\n",
        "            config = LoraConfig(\n",
        "                r=lora_r,\n",
        "                lora_alpha=lora_alpha,\n",
        "                target_modules=lora_target_modules,\n",
        "                lora_dropout=lora_dropout,\n",
        "                bias=\"none\", #the adaptation does not affect bias terms in the target modules.\n",
        "                task_type=\"CAUSAL_LM\", # setting up LoRA in models designed for text generation\n",
        "            )\n",
        "            model = get_peft_model(model, config)\n",
        "\n",
        "        self.model = model\n",
        "        print('Finish loading LLAMA!')\n",
        "\n",
        "\n",
        "\n",
        "        #GNN configuration for generate embedding\n",
        "        self.graph_encoder = load_gnn_model[args.gnn_model_name](\n",
        "            in_channels=args.gnn_in_dim,\n",
        "            out_channels=args.gnn_hidden_dim,\n",
        "            hidden_channels=args.gnn_hidden_dim,\n",
        "            num_layers=args.gnn_num_layers,\n",
        "            dropout=args.gnn_dropout,\n",
        "            num_heads=args.gnn_num_heads,\n",
        "        ).to(self.model.device)\n",
        "\n",
        "\n",
        "        ## Projection layer to maps the GNN's graph embeddings to a size compatible with the LLM's input embeddings.\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(args.gnn_hidden_dim, 2048),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(2048, 4096),\n",
        "        ).to(self.model.device)\n",
        "\n",
        "        #Accesses the pre-trained word embedding layer from the LLM.\n",
        "        self.word_embedding = self.model.model.get_input_embeddings()\n",
        "\n",
        "    @property # makes methods look and behave like attributes.\n",
        "\n",
        "    def device(self):\n",
        "        return list(self.parameters())[0].device\n",
        "\n",
        "    def maybe_autocast(self, dtype=torch.bfloat16):#reduces memory usage and accelerates computations on GPUs.\n",
        "        # if on cpu, don't use autocast\n",
        "        # if on gpu, use autocast with dtype if provided, otherwise use torch.float16\n",
        "        enable_autocast = self.device != torch.device(\"cpu\")\n",
        "\n",
        "        if enable_autocast:\n",
        "            return torch.cuda.amp.autocast(dtype=dtype)\n",
        "        else:\n",
        "            return contextlib.nullcontext()\n",
        "\n",
        "    def encode_graphs(self, samples):\n",
        "        graphs = samples['graph']\n",
        "        graphs = graphs.to(self.model.device)\n",
        "        n_embeds, _ = self.graph_encoder(graphs.x, graphs.edge_index.long(), graphs.edge_attr) #forward gt class\n",
        "\n",
        "        # mean pooling\n",
        "        g_embeds = scatter(n_embeds, graphs.batch, dim=0, reduce='mean') #n_embeds :[num_nodes, embedding_dim],\n",
        "\n",
        "        # graphs.batch: A tensor of size [num_nodes] mapping each node to the graph it belongs to in the batch.\n",
        "        # Example: If there are 6 nodes in total from 3 graphs:\n",
        "        # css\n",
        "        # Copy code\n",
        "        # graphs.batch = [0, 0, 1, 1, 2, 2]\n",
        "        # Nodes 0 and 1 belong to graph 0.\n",
        "        # Nodes 2 and 3 belong to graph 1.\n",
        "        # Nodes 4 and 5 belong to graph 2.\n",
        "\n",
        "        return g_embeds\n",
        "\n",
        "    ##Generation\n",
        "    def forward(self, samples):\n",
        "\n",
        "        # encode description, questions and labels\n",
        "        questions = self.tokenizer(samples[\"question\"], add_special_tokens=False)\n",
        "        descriptions = self.tokenizer(samples[\"desc\"], add_special_tokens=False)\n",
        "        labels = self.tokenizer(samples[\"label\"], add_special_tokens=False)\n",
        "\n",
        "        # encode special tokens\n",
        "        eos_tokens = self.tokenizer(EOS, add_special_tokens=False)\n",
        "        eos_user_tokens = self.tokenizer(EOS_USER, add_special_tokens=False)\n",
        "        bos_embeds = self.word_embedding(self.tokenizer(BOS, add_special_tokens=False, return_tensors='pt').input_ids[0])\n",
        "        pad_embeds = self.word_embedding(torch.tensor(self.tokenizer.pad_token_id)).unsqueeze(0)\n",
        "\n",
        "        # encode graphs\n",
        "        graph_embeds = self.encode_graphs(samples)\n",
        "        graph_embeds = self.projector(graph_embeds)\n",
        "\n",
        "        batch_size = len(samples['id'])\n",
        "        batch_inputs_embeds = []\n",
        "        batch_attention_mask = []\n",
        "        batch_label_input_ids = []\n",
        "        for i in range(batch_size):\n",
        "            # Add bos & eos token\n",
        "            label_input_ids = labels.input_ids[i][:self.max_new_tokens] + eos_tokens.input_ids\n",
        "            input_ids = descriptions.input_ids[i][:self.max_txt_len] + questions.input_ids[i] + eos_user_tokens.input_ids + label_input_ids\n",
        "            inputs_embeds = self.word_embedding(torch.tensor(input_ids).to(self.model.device))\n",
        "            inputs_embeds = torch.cat([bos_embeds, graph_embeds[i].unsqueeze(0), inputs_embeds], dim=0)\n",
        "\n",
        "            batch_inputs_embeds.append(inputs_embeds)\n",
        "            batch_attention_mask.append([1] * inputs_embeds.shape[0])\n",
        "            label_input_ids = [IGNORE_INDEX] * (inputs_embeds.shape[0]-len(label_input_ids))+label_input_ids\n",
        "            batch_label_input_ids.append(label_input_ids)\n",
        "\n",
        "        # pad inputs_embeds\n",
        "        max_length = max([x.shape[0] for x in batch_inputs_embeds])\n",
        "        for i in range(batch_size):\n",
        "            pad_length = max_length-batch_inputs_embeds[i].shape[0]\n",
        "            batch_inputs_embeds[i] = torch.cat([pad_embeds.repeat(pad_length, 1), batch_inputs_embeds[i]])\n",
        "#pad_embeds.repeat(pad_length, 1):\n",
        "# tensor([\n",
        "#     [0.1, 0.2, 0.3],  # Copy 1\n",
        "#     [0.1, 0.2, 0.3],  # Copy 2\n",
        "#     [0.1, 0.2, 0.3],  # Copy 3\n",
        "#     [0.1, 0.2, 0.3],  # Copy 4\n",
        "# ])  # Shape: [4, 3]\n",
        "            batch_attention_mask[i] = [0]*pad_length+batch_attention_mask[i]\n",
        "            batch_label_input_ids[i] = [IGNORE_INDEX] * pad_length+batch_label_input_ids[i]\n",
        "\n",
        "        inputs_embeds = torch.stack(batch_inputs_embeds, dim=0).to(self.model.device)\n",
        "        attention_mask = torch.tensor(batch_attention_mask).to(self.model.device)\n",
        "        label_input_ids = torch.tensor(batch_label_input_ids).to(self.model.device)\n",
        "\n",
        "        with self.maybe_autocast():\n",
        "            outputs = self.model(\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                attention_mask=attention_mask,\n",
        "                return_dict=True,\n",
        "                labels=label_input_ids,\n",
        "            )\n",
        "\n",
        "        return outputs.loss\n",
        "\n",
        "    def inference(self, samples):\n",
        "\n",
        "        # encode description and questions\n",
        "        questions = self.tokenizer(samples[\"question\"], add_special_tokens=False)\n",
        "        descriptions = self.tokenizer(samples[\"desc\"], add_special_tokens=False)\n",
        "\n",
        "        # encode special tokens\n",
        "        eos_user_tokens = self.tokenizer(EOS_USER, add_special_tokens=False)\n",
        "        bos_embeds = self.word_embedding(self.tokenizer(BOS, add_special_tokens=False, return_tensors='pt').input_ids[0])\n",
        "        pad_embeds = self.word_embedding(torch.tensor(self.tokenizer.pad_token_id)).unsqueeze(0)\n",
        "\n",
        "        # encode graphs\n",
        "        graph_embeds = self.encode_graphs(samples)\n",
        "        graph_embeds = self.projector(graph_embeds)\n",
        "\n",
        "        batch_size = len(samples['id'])\n",
        "        batch_inputs_embeds = []\n",
        "        batch_attention_mask = []\n",
        "        for i in range(batch_size):\n",
        "            # Add bos & eos token\n",
        "            input_ids = descriptions.input_ids[i][:self.max_txt_len] + questions.input_ids[i] + eos_user_tokens.input_ids\n",
        "            inputs_embeds = self.word_embedding(torch.tensor(input_ids).to(self.model.device))\n",
        "            inputs_embeds = torch.cat([bos_embeds, graph_embeds[i].unsqueeze(0), inputs_embeds], dim=0)\n",
        "            batch_inputs_embeds.append(inputs_embeds)\n",
        "            batch_attention_mask.append([1] * inputs_embeds.shape[0])\n",
        "\n",
        "        # pad inputs_embeds\n",
        "        max_length = max([x.shape[0] for x in batch_inputs_embeds])\n",
        "        for i in range(batch_size):\n",
        "            pad_length = max_length-batch_inputs_embeds[i].shape[0]\n",
        "            batch_inputs_embeds[i] = torch.cat([pad_embeds.repeat(pad_length, 1), batch_inputs_embeds[i]])\n",
        "            batch_attention_mask[i] = [0]*pad_length+batch_attention_mask[i]\n",
        "\n",
        "        inputs_embeds = torch.stack(batch_inputs_embeds, dim=0).to(self.model.device)\n",
        "        attention_mask = torch.tensor(batch_attention_mask).to(self.model.device)\n",
        "\n",
        "        with self.maybe_autocast():\n",
        "            outputs = self.model.generate(\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                attention_mask=attention_mask,\n",
        "                # do_sample=True,\n",
        "                use_cache=True  # IMPORTANT!\n",
        "            )\n",
        "        pred = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        return {'id': samples['id'],\n",
        "                'pred': pred,\n",
        "                'label': samples['label'],\n",
        "                'question': samples['question'],\n",
        "                'desc': samples['desc'], }\n",
        "\n",
        "    def print_trainable_params(self):\n",
        "        trainable_params = 0\n",
        "        all_param = 0\n",
        "\n",
        "        for _, param in self.named_parameters():\n",
        "            num_params = param.numel()\n",
        "\n",
        "            all_param += num_params\n",
        "            if param.requires_grad:\n",
        "                trainable_params += num_params\n",
        "\n",
        "        return trainable_params, all_param"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sadafenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
