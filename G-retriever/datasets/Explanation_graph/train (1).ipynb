{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QrW2ReL_oea"
      },
      "outputs": [],
      "source": [
        "import random, os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "      \"\"\"\n",
        "    Set a fixed random seed for reproducibility across multiple libraries and environments.\n",
        "\n",
        "    This function ensures that experiments in machine learning and data science are reproducible\n",
        "    by setting the same random seed for various sources of randomness, including Python's `random` module,\n",
        "    NumPy, and PyTorch (for both CPU and GPU). It also configures PyTorch's CuDNN backend to enforce\n",
        "    deterministic behavior.\n",
        "\n",
        "    Args:\n",
        "        seed (int): The seed value to set for all random number generators.\n",
        "\n",
        "    Libraries Affected:\n",
        "        - `random`: Sets the random seed for Python's built-in random module.\n",
        "        - `os`: Sets the `PYTHONHASHSEED` environment variable to ensure deterministic hashing in Python.\n",
        "        - `numpy`: Sets the random seed for NumPy.\n",
        "        - `torch`: Sets the seed for PyTorch's random number generation on both CPU and GPU.\n",
        "\n",
        "    PyTorch-Specific Settings:\n",
        "        - `torch.backends.cudnn.deterministic = True`:\n",
        "            Forces PyTorch to use deterministic algorithms for CuDNN operations.\n",
        "        - `torch.backends.cudnn.benchmark = True`:\n",
        "            Enables dynamic algorithm optimization in CuDNN for faster execution in some cases.\n",
        "\n",
        "    Example:\n",
        "        >>> seed_everything(42)\n",
        "        >>> random.randint(0, 10)  # Always generates the same number\n",
        "        >>> np.random.rand(3)  # Produces the same array for a fixed seed\n",
        "        >>> torch.randn(3, 3)  # Produces the same tensor for a fixed seed\n",
        "\n",
        "    Notes:\n",
        "        - While `torch.backends.cudnn.deterministic = True` ensures reproducibility, it may slow down computations.\n",
        "        - `torch.backends.cudnn.benchmark = True` may introduce slight variability in speed for different input sizes.\n",
        "\n",
        "    \"\"\"\n",
        "    random.seed(seed) #for random library\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed) #for numpy\n",
        "    torch.manual_seed(seed) #for cpu\n",
        "    torch.cuda.manual_seed(seed)#for gpu\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8j_meI__N4C"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Batch\n",
        "\n",
        "\n",
        "def collate_fn(original_batch):\n",
        "  \"\"\"\n",
        "\n",
        "    Custom collate function for batching data in a PyTorch DataLoader.\n",
        "\n",
        "    This function processes a batch of individual samples (dictionaries) from the dataset and combines them into a\n",
        "    single batch dictionary. If the data contains graphs (PyTorch Geometric `Data` objects), they are batched\n",
        "    into a single graph using `Batch.from_data_list`.\n",
        "\n",
        "    Args:\n",
        "        original_batch (list of dict): A batch of samples from the dataset. Each sample is a dictionary where keys\n",
        "                                        represent feature names (e.g., \"question\", \"labels\", \"desc\", \"graph\"), and values\n",
        "                                        are the corresponding data for each sample.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing batched data for all keys:\n",
        "              - For non-graph data, values are lists of the corresponding data from each sample.\n",
        "              - For graph data (key: \"graph\"), a single `Batch` object from PyTorch Geometric is returned.\n",
        "\n",
        "              Example output:\n",
        "              {\n",
        "                  \"input_ids\": [[101, 200, 300], [102, 201, 301]],\n",
        "                  \"labels\": [1, 0],\n",
        "                  \"graph\": Batch(...)  # Batched PyTorch Geometric graph object\n",
        "              }\n",
        "\n",
        "    Notes:\n",
        "        - If the dataset contains a \"graph\" key, it is expected to be a PyTorch Geometric `Data` object.\n",
        "        - Non-graph features are combined into lists for easier downstream processing.\n",
        "\n",
        "  \"\"\"\n",
        "    batch = {}\n",
        "    for k in original_batch[0].keys():\n",
        "        batch[k] = [d[k] for d in original_batch]\n",
        "    if 'graph' in batch:\n",
        "        batch['graph'] = Batch.from_data_list(batch['graph'])\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDh4e8Td_QFC"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "def adjust_learning_rate(param_group, LR, epoch, args):\n",
        "    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n",
        "    min_lr = 5e-6\n",
        "    if epoch < args.warmup_epochs: #In epoch 1, the learning rate starts at 0 and gradually increases to LR over the warmup_epochs.\n",
        "        lr = LR * epoch / args.warmup_epochs\n",
        "    else:\n",
        "      #After warmup_epochs, the learning rate decreases to min_lr following a half-cycle cosine schedule.\n",
        "        lr = min_lr + (LR - min_lr) * 0.5 * (1.0 + math.cos(math.pi * (epoch - args.warmup_epochs) / (args.num_epochs - args.warmup_epochs)))\n",
        "    param_group[\"lr\"] = lr\n",
        "    return lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnTF-Hiz_VMK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "def print_trainable_params(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "\n",
        "    for _, param in model.named_parameters():\n",
        "        num_params = param.numel()\n",
        "\n",
        "        all_param += num_params\n",
        "        if param.requires_grad:\n",
        "            trainable_params += num_params\n",
        "\n",
        "    return trainable_params, all_param\n",
        "\n",
        "\n",
        "def _save_checkpoint(model, optimizer, cur_epoch, args, is_best=False):\n",
        "    \"\"\"\n",
        "    Save the checkpoint at the current epoch.\n",
        "    \"\"\"\n",
        "    os.makedirs(f'{args.output_dir}/{args.dataset}', exist_ok=True)\n",
        "\n",
        "    param_grad_dic = {\n",
        "        k: v.requires_grad for (k, v) in model.named_parameters()\n",
        "    }\n",
        "    state_dict = model.state_dict() #learnable parameter\n",
        "    for k in list(state_dict.keys()):\n",
        "        if k in param_grad_dic.keys() and not param_grad_dic[k]:\n",
        "            # delete parameters that do not require gradient\n",
        "            del state_dict[k]\n",
        "    save_obj = {\n",
        "        \"model\": state_dict,\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"config\": args,\n",
        "        \"epoch\": cur_epoch,\n",
        "    }\n",
        "    path = f'{args.output_dir}/{args.dataset}/model_name_{args.model_name}_llm_model_name_{args.llm_model_name}_llm_frozen_{args.llm_frozen}_max_txt_len_{args.max_txt_len}_max_new_tokens_{args.max_new_tokens}_gnn_model_name_{args.gnn_model_name}_patience_{args.patience}_num_epochs_{args.num_epochs}_seed{args.seed}_checkpoint_{\"best\" if is_best else cur_epoch}.pth'\n",
        "    print(\"Saving checkpoint at epoch {} to {}.\".format(cur_epoch, path))\n",
        "    torch.save(save_obj, path)\n",
        "\n",
        "\n",
        "def _reload_best_model(model, args):\n",
        "    \"\"\"\n",
        "    Load the best checkpoint for evaluation.\n",
        "    \"\"\"\n",
        "    checkpoint_path = f'{args.output_dir}/{args.dataset}/model_name_{args.model_name}_llm_model_name_{args.llm_model_name}_llm_frozen_{args.llm_frozen}_max_txt_len_{args.max_txt_len}_max_new_tokens_{args.max_new_tokens}_gnn_model_name_{args.gnn_model_name}_patience_{args.patience}_num_epochs_{args.num_epochs}_seed{args.seed}_checkpoint_best.pth'\n",
        "\n",
        "    print(\"Loading checkpoint from {}.\".format(checkpoint_path))\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
        "    model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _reload_model(model, checkpoint_path):\n",
        "    \"\"\"\n",
        "    Load the best checkpoint for evaluation.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Loading checkpoint from {}.\".format(checkpoint_path))\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
        "    model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ4pc_zc_c--"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "def get_accuracy_expla_graphs(path):\n",
        "    df = pd.read_json(path, lines=True)\n",
        "    # compute accuracy\n",
        "    correct = 0\n",
        "    for pred, label in zip(df[\"pred\"], df[\"label\"]):\n",
        "        matches = re.findall(r\"support|Support|Counter|counter\", pred.strip())\n",
        "        if len(matches) > 0 and matches[0].lower() == label:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRpZTtr__zrA"
      },
      "outputs": [],
      "source": [
        "eval_funcs = {\n",
        "    \"expla_graphs\": get_accuracy_expla_graphs,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XlnH6npRG1I"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import wandb\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# from src.model import load_model, llama_model_path\n",
        "# from src.dataset import load_dataset\n",
        "# from src.utils.evaluate import eval_funcs\n",
        "# from src.config import parse_args_llama\n",
        "# from src.utils.ckpt import _save_checkpoint, _reload_best_model\n",
        "# from src.utils.collate import collate_fn\n",
        "# from src.utils.seed import seed_everything\n",
        "# from src.utils.lr_schedule import adjust_learning_rate\n",
        "\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    # Step 1: Set up wandb\n",
        "    seed = args.seed\n",
        "    wandb.init(project=f\"{args.project}\",\n",
        "               name=f\"{args.dataset}_{args.model_name}_seed{seed}\",\n",
        "               config=args)\n",
        "\n",
        "    seed_everything(seed=args.seed)\n",
        "    print(args)\n",
        "\n",
        "    dataset = load_dataset[args.dataset]() #load Explanation_graph\n",
        "    idx_split = dataset.get_idx_split() #train-val-test index\n",
        "\n",
        "    # Step 2: Build Node Classification Dataset\n",
        "    train_dataset = [dataset[i] for i in idx_split['train']] #contains label, desc, question, graph...\n",
        "    val_dataset = [dataset[i] for i in idx_split['val']]\n",
        "    test_dataset = [dataset[i] for i in idx_split['test']]\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, drop_last=True, pin_memory=True, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, drop_last=False, pin_memory=True, shuffle=False, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=args.eval_batch_size, drop_last=False, pin_memory=True, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # Step 3: Build Model\n",
        "    args.llm_model_path = llama_model_path[args.llm_model_name] #edumunozsala/llama-2-7b-int4-python-code-20k model\n",
        "\n",
        "    model = load_model[args.model_name](graph_type=dataset.graph_type, args=args, init_prompt=dataset.prompt)#load graph_llm\n",
        "\n",
        "    # Step 4 Set Optimizer\n",
        "    params = [p for _, p in model.named_parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        [{'params': params, 'lr': args.lr, 'weight_decay': args.wd}, ],\n",
        "        betas=(0.9, 0.95)\n",
        "    )\n",
        "\n",
        "\n",
        "    trainable_params, all_param = model.print_trainable_params()\n",
        "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n",
        "\n",
        "    # Step 5. Training\n",
        "    num_training_steps = args.num_epochs * len(train_loader)\n",
        "    progress_bar = tqdm(range(num_training_steps))\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(args.num_epochs):\n",
        "\n",
        "        model.train()\n",
        "        epoch_loss, accum_loss = 0., 0.\n",
        "\n",
        "        for step, batch in enumerate(train_loader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = model(batch)\n",
        "            loss.backward()\n",
        "\n",
        "            clip_grad_norm_(optimizer.param_groups[0]['params'], 0.1) # Limits the gradient norm to a maximum of 0.1 to prevent exploding gradients.\n",
        "\n",
        "            if (step + 1) % args.grad_steps == 0: #This condition ensures that the learning rate adjustment only occurs after a specific number of gradient accumulation steps\n",
        "                adjust_learning_rate(optimizer.param_groups[0], args.lr, step / len(train_loader) + epoch, args) # step / len(train_loader) + epoch : The learning rate is adjusted smoothly not only at the start of each epoch but continuously during the epoch as well.\n",
        "\n",
        "            optimizer.step() #update weight\n",
        "            epoch_loss, accum_loss = epoch_loss + loss.item(), accum_loss + loss.item()\n",
        "\n",
        "\n",
        "            #Log to wandb\n",
        "            if (step + 1) % args.grad_steps == 0:\n",
        "                lr = optimizer.param_groups[0][\"lr\"]\n",
        "                wandb.log({'Lr': lr})\n",
        "                wandb.log({'Accum Loss': accum_loss / args.grad_steps})\n",
        "                accum_loss = 0.\n",
        "\n",
        "\n",
        "            progress_bar.update(1)\n",
        "\n",
        "        print(f\"Epoch: {epoch}|{args.num_epochs}: Train Loss (Epoch Mean): {epoch_loss / len(train_loader)}\")\n",
        "        wandb.log({'Train Loss (Epoch Mean)': epoch_loss / len(train_loader)})\n",
        "\n",
        "\n",
        "#ŸèStep6 : evaluation on validation\n",
        "        val_loss = 0.\n",
        "        eval_output = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for step, batch in enumerate(val_loader):\n",
        "                loss = model(batch)\n",
        "                val_loss += loss.item()\n",
        "            val_loss = val_loss/len(val_loader)\n",
        "            print(f\"Epoch: {epoch}|{args.num_epochs}: Val Loss: {val_loss}\")\n",
        "            wandb.log({'Val Loss': val_loss})\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            _save_checkpoint(model, optimizer, epoch, args, is_best=True)\n",
        "            best_epoch = epoch\n",
        "\n",
        "        print(f'Epoch {epoch} Val Loss {val_loss} Best Val Loss {best_val_loss} Best Epoch {best_epoch}')\n",
        "\n",
        "        if epoch - best_epoch >= args.patience:\n",
        "            print(f'Early stop at epoch {epoch}')\n",
        "            break\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "\n",
        "    # Step 7. Evaluating on test with infernece part\n",
        "    os.makedirs(f'{args.output_dir}/{args.dataset}', exist_ok=True)\n",
        "    path = f'{args.output_dir}/{args.dataset}/model_name_{args.model_name}_llm_model_name_{args.llm_model_name}_llm_frozen_{args.llm_frozen}_max_txt_len_{args.max_txt_len}_max_new_tokens_{args.max_new_tokens}_gnn_model_name_{args.gnn_model_name}_patience_{args.patience}_num_epochs_{args.num_epochs}_seed{seed}.csv'\n",
        "    print(f'path: {path}')\n",
        "\n",
        "    model = _reload_best_model(model, args)\n",
        "    model.eval()\n",
        "    progress_bar_test = tqdm(range(len(test_loader)))\n",
        "    with open(path, \"w\") as f:\n",
        "        for step, batch in enumerate(test_loader):\n",
        "            with torch.no_grad():\n",
        "                output = model.inference(batch)\n",
        "                df = pd.DataFrame(output)\n",
        "                for _, row in df.iterrows():\n",
        "                    f.write(json.dumps(dict(row)) + \"\\n\")\n",
        "            progress_bar_test.update(1)\n",
        "\n",
        "    # Step 6. Post-processing & compute metrics\n",
        "    acc = eval_funcs[args.dataset](path)\n",
        "    print(f'Test Acc {acc}')\n",
        "    wandb.log({'Test Acc': acc})\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    args = parse_args_llama()\n",
        "\n",
        "\n",
        "    main(args)\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    gc.collect()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sadafenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
